\documentclass{article}

\input{commands}
\usepackage{parskip}
\usepackage{natbib}

\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\begin{document}

\tableofcontents

% Change Font and Spacing
\large % change the font size to 12pt
% \linespread{1.1} % change the line spacing

\section{Regarding the mixture of Betas}

This shows why knowing calculus well is important.

Suppose you have a mixture of Betas representation
\begin{align*}
f(x) 
& = \int_0^\infty \frac{1}{y} \Big( 1 - \frac{x}{ky} \Big)^{k-1}_+ dG(y), \; k > 1; \\
& = \int_0^\infty \frac{1}{y} \exp(-x/y) dG(y), \; k = \infty.
\end{align*}
Then you can solve for $G$ or $g = G'$ in terms of $f$, which tells you how to
construct a mixture.  Note that the support of $y$ $(0, \infty)$.  The mixture
$p(x|y)$ is $\beta(1, k)$ with support on $(0, ky)$.

Focusing on the finite $k$ case.  Since the $(\cdot)_+$ function is zero for $y
< x/k$ we know that the support is on $y > x/k$.
\[
f(x) = \int_{x/k}^\infty y^{-1}  \Big( 1 - \frac{x}{ky} \Big)^{k-1}_+ g(y) dy.
\]
I have gone ahead and assumed that $G$ is differentiable, or differentiable
almost everywhere or whatever is required.

Something we need to remind ourselves.  If 
\[
f(x) = \int_0^x g(x,y) dy
\]
then
\[
f'(x) = g(x,x) + \int_0^x g_x(x,y) dy
\]
given that $g$ is differentiable (implying continuity) and we have some DCT to
take the derivative inside the integral.  Impressionistically,
\[
\frac{1}{\ep} \Big( \int_0^{x+\ep} g(x+\ep, y) dy - \int_0^{x} g(x, y) dy \Big)
= \frac{1}{\ep} \int_x^{x+\ep} g(x+\ep, y) dy
+ \int_0^x \frac{g(x+\ep, y) - g(x,y)}{\ep} dy.
\]
Using the continuity of $g$ and some sort of DCT we can take the limit and get
our claim.

When $k=1$, we don't even have to worry about that.  In that case,
\[
f(x) = \int_x^\infty \frac{1}{y} g(y) dy.
\]
Then \( f'(x) = - g(x) / x \), which becomes
\[
g(x) = -x f'(x).
\]
Integrating to get $G$ and using integration by parts for $f$ we have
\[
\int_0^x g(z) dz = \int_0^x -z f'(z) dz = -z f(z) \mid_{0}^x + \int_0^x f(z) dz.
\]
Thus
\[
G(x) = -x f(x) + F(x).
\]

When $k=2$ we have
\[
f(x) = \int_{x/2}^\infty \frac{1}{y} \Big(1 - \frac{x}{2y}\Big)_+ g(y) dy.
\]
Differentiating we have
\[
f'(x) = 0 + \int_{x/2}^\infty \frac{1}{y} \Big( \frac{-1}{2y} \Big) g(y) dy.
\]
And differentiating again
\[
f''(x) = \frac{2}{x^2} g(x/2),
\]
which becomes (letting $\bar g(x) = g(x/2)$)
\[
\bar g(x) = \frac{x^2}{2} f''(x).
\]
Now integrating and then integrating by parts we have
\begin{align*}
\int_0^x \bar g(z) dz & = \int_0^x \frac{z^2}{2} f''(z) dz \\
& = \frac{z^2}{2} f'(z) \mid_{0}^x - \int_0^x z f'(z) dz \\
& = \frac{-x^2}{2} f'(x) - z f(z) \mid_{0}^x + \int_0^x f(z) dz.
\end{align*}
Hence
\[
G(x/2) = F(x) - x F'(x) + \frac{x^2}{2} F''(x).
\]
We know that $G$ is a CDF because $F$ is going to go to unity and the
derivatives will vanish (assuming that they do and at a sufficiently fast rate).

By our $k$-monotone condition, I think we know that we can take this many
derivatives and that they are decaying as fast as we want them to.

In general, the $k-1$st derivative of a $k$-monotone function is going to be
(think about differentiaing under the integral)
\[
\int_0^\infty \frac{(k-1)!}{(ky)^{k-1}} \bbI\{ky > x > 0\} \frac{1}{y} g(y) dy
=
\int_{x/k}^\infty (-1)^{k-1} \frac{(k-1)!}{(ky)^{k-1}} \frac{1}{y} g(y) dy.
\]
Thus the $k$th derivative is
\[
f^{(k)}(x) = (-1) (-1)^{k-1} \frac{(k-1)!}{x^{k-1}} \frac{k}{x} g(x/k)
\]
from which we see that
\[
f^{(k)}(x) = (-1)^k \frac{k!}{x^k} g(x/k)
\]
and hence
\[
g(x/k) = (-1)^k \frac{x^k}{k!} f^{(k)}(x).
\]
Doing the integration by parts trick will get us the CDF for $G$, which can be
described by
\[
G(x/k) = \sum_{i=0}^k \frac{(-1)^i}{i!} x^i F^{(i)} (x).
\]

\section{Expectation Maximization Refresher}

Again, I need to remind myself about the EM algorithm.  It is worth reading
\cite{dempster-etal-1977}.  They motivate their work using exponential families.
That is actually more difficult to understand than the auxiliary variable setup
where you have data and an auxilliary variable.

For the sake of brevity we simply review the simple version.  You have data $y$,
unknown parameters $\theta$, and an auxiliary variable $\omega$.  You want to
find the maximum likelihood.  By probability calculus
\begin{displaymath}
\label{eqn:marginal}
p(y|\theta) = \frac{p(y,\omega | \theta)}{p(\omega | y, \theta)}.
\end{displaymath}
Taking the logarithm of both sides, which preserves the mode, yields
\[
\ln p(y | \theta) = \ln p(y,\omega | \theta) - \ln p(\omega | y, \theta).
\]
The key to everything is in the last term.  If we take the expectation with
respect to $p(\omega | y, \theta^p)$, then
\[
H(\theta | \theta^p) = \bbE [ \ln p(\omega | y, \theta) | y, \theta^p ]
\]
has its maximum at $\theta = \theta^p$.  If we apply this expectation to both
sides we have
\[
\ln p(y | \theta) = Q(\theta | \theta^p) - H(\theta | \theta^p)
\]
where
\[
Q(\theta | \theta^p) = \bbE[ \ln p(y,\omega | \theta) | y, \theta^p ]
\]
Thus if we are at $\theta^p$ we can chose any $\theta$ so that $Q(\theta |
\theta^p) > Q(\theta^p | \theta^p)$ to increase the value of $\ln p(y | theta)$.
We can maximize $Q$ given $\theta^p$ to increase the log-likelihood as much as
possible in that one step.  In this way one can construct a sequence $\theta^1,
\theta^2, \ldots$ that coverge to $\theta^*$ the maximum likelihood.  The
function $Q$ is called the complete data log-likelihood because $p(y, \omega |
\theta)$ is the likelihood for the ``complete'' observation $(y, \omega)$, which
is only ``partially'' observed when one aquires $y$.

The same applies for finding the posterior mode, except that now we start with
\[
p(\theta | y) = \frac{p(\theta, \omega | y)}{p(\omega | \theta, y)}.
\]
The $H$ term is idential and thus possesses the same property, but now we have
\[
Q(\theta | \theta^p) = \bbE[ \ln p(\theta, \omega | y) | \theta^p, y ].
\]

\section{Regarding EM for BR with triangles}

Regarding bridge regression by augmentation: the prior can be prepresnted as
\[
p(\beta) = \int_0^\infty p(\beta | \omega) p(\omega) d \omega
\]
where $p(\beta | \omega)$ is 
\[
\propto \frac{1}{2} \frac{1}{\omega} \Big( 1 - \frac{|\beta|}{k\omega}
\Big)^{k-1}_+.
\]
This density has support on $(-k\omega, k \omega)$ in contrast to the case above
where we looked at support on $(0, k \omega)$.

We need two quantities:
\[
p(\beta, \omega | y) = \frac{p(y|\beta) p(\beta | \omega) p(\omega)}{p(y)}.
\]
Taking the logarithm we have
\[
\ln p(y | \beta) + \ln p(\beta | \omega) + \ln p(\omega) - \ln p(y).
\]
Only the first two terms will vary with $\beta$ after taking the expectation, thus
\[
Q(\beta | \beta^p) = \underbrace{\ln p(y | \beta) + \bbE[\ln p(\beta | \omega) |
  \beta^p, y]}_{\mcQ} + K
\]
where the expectation is taken with respect to
\[
p(\omega | \beta^p, y) \propto p(\beta^p | \omega) p(\omega).
\]

I believe Polson is saying that we will be using the $\alpha$-stable prior for
each coordinate of $\beta$.  Thus the prior would be
\[
\propto \prod_{j=1}^P \exp(-|\beta_j|^\alpha).
\]
Thus we need an auxiliary variable for each coordinate.  This is in contrast to
the case where the prior is
\[
\propto \exp (-|\beta|^\alpha)
\]
which is spherically symmetric.  I think one may be able to proceed in this
fashion actually.  In the first case,
\[
p(\beta | \omega) = \sum_{j=1}^P \frac{1}{\omega_j} \Big(1 -
\frac{|\beta_j|}{k\omega_j}\Big)^{k-1}_+.
\]
In the second case,
\[
p(\beta | \omega) \propto \frac{1}{\omega} \Big(1 -
\frac{|\beta|}{k\omega}\Big)_+^{k-1}.
\]
I need to check what the constant of proportionality is here.  I think there is
a chance it does not depend on $\omega$.  (Do the change $\gamma = \beta /
k\omega$.  Then use polar coordinates.  I think you get a beta distribution in
the radius and there are no $\omega$s left.)  Though come to think of it, maybe
this prior is not what you want.  You shrink towards zero, but the spherical
symmetry won't favor one variable over another.

In either event $p(\beta | \omega)$ is unimodal, though the level sets have
different symmetries.

To derive the density $p(\omega | \alpha)$ for the mixture when $k=2$ we have
$f(x) \propto \exp(-x^\alpha)$ so that
\[
f''(x) 
% f'(x) = - \alpha x^{\alpha - 1} \exp(-x^\alpha)
\propto - \alpha (\alpha - 1) x^{\alpha - 2} \exp({-x^\alpha}) + \alpha^2
x^{2\alpha - 2} \exp(-x^\alpha).
\]
so that
\[
\frac{x^2}{2} f''(x) \propto \frac{1}{2} \Big[ \alpha (1 - \alpha) x^{\alpha}
\exp({-x^\alpha}) + \alpha^2 x^{2\alpha} \exp(-x^\alpha) \Big].
\]
We can plug in $x := 2x$ and do a change of variables $\xi = (2x)^\alpha$
(that yields $dx = \frac{1}{2 \alpha} \xi^{1/\alpha - 1}$) to get
\[
p(\xi | \alpha) \propto \alpha \xi^{2 + \frac{1}{\alpha} - 1} e^{-\xi} + 
(1-\alpha) \xi^{1 + \frac{1}{\alpha} - 1} e^{-\xi}.
\]
Then
\[
p(-|\beta|^\alpha) \propto \int_0^\infty \frac{1}{\xi^{1/\alpha}} \Big( 1 -
\frac{|\beta|}{2 \xi^{1/\alpha}} \Big) p(\xi | \alpha) d\xi.
\]
Here $\beta \in \bbR$.

I think this aligns with what is in the notes because they take the $1/\omega$
and bring it into $p(\omega | \alpha)$.

I'm not sure how you calculate
\[
\int z^{1/\alpha} (1 - |\beta| z^{-1/\alpha}) p(z | \beta^p, \alpha).
\]

Looking at James and Nick's paper though it appears that they actually use the
normal scale-mixture of stable distributions for the EM.

\section{Mixture of Normals MCMC}

If we let $S_\alpha$ be the stable distribution with parameter $\alpha$,
following the notation of \cite{devroye-2009}, then as found on p. 1 of his
paper
\[
\bbE[\exp(-z S_\alpha)] = \exp(-z^\alpha), \; z \geq 0, \alpha \in (0, 1].
\]

James and Nick want a scale mixture of normal.  They have
\[
\exp(-|t|^\gamma) = \bbE[ \exp(-\frac{t^2}{2} \Lambda) ], \; \gamma \in (0, 2],
\]
where $\Lambda$ is related to a stable distribution.  By letting $z = y^2$ we
have
\[
\bbE[\exp(-\frac{y^2}{2} 2 S_\alpha)] = \exp(-(y^2)^\alpha)
= \exp( - |y|^{2 \alpha} ).
\]
So we can reconcile the two by letting $\gamma = 2 \alpha$ and $\Lambda = 2
S_{\gamma / 2}$ to get
\[
\bbE[\exp(-\frac{y^2}{2} 2 S_{\gamma / 2})] = \exp( - |y|^{\gamma} ), \; \gamma
\in (0, 2].
\]

In bridge regression we have a prior
\[
p(\beta) = C(\gamma) \prod_{i=1}^p \exp (- |\beta_i / \tau|^\gamma ),
\]
which can be written as
\[
C(\gamma) \prod_{i=1}^p \bbE \Big[ \exp( - \frac{\beta_i^2}{2 \tau^2}
\lambda_i ) \Big], \; \lambda_i \sim 2 S_{\gamma / 2}
\]
and then ``demarginalized'' as
\[
p(\beta, \lambda) = C(\gamma) 
\prod_{i=1}^p \exp(- \frac{\beta_i^2}{2 \tau^2} \lambda_i) p(\lambda_i)
\]
where, again, $p(\lambda_i)$ is the density of $2 S_{\gamma/2}$.

Thus by Bayes Theorem
\[
p(\beta, \lambda | y) \propto p(y | \beta) p(\beta, \lambda)
\]
since the likelihood doesn't involve $\lambda$.  Writing this out we have that
\[
\propto
\exp \Big[ -\frac{1}{2 \sigma^2} (y - X \beta)'(y - X \beta) \Big]
\prod_{i=1}^p \exp(- \frac{\beta_i^2}{2 \tau^2} \lambda_i) p(\lambda_i)
\]
or
\[
\exp \Big\{ \frac{-1}{2 \sigma^2} \Big[ \beta' X'X \beta - 2 \beta' X' y \Big]
+ \frac{-1}{2 \tau^2} \beta' \Lambda \beta \Big\} \prod_{i=1}^p p(\lambda_i).
\]

\subsection{Sampling $p(\lambda | \beta, \sigma^2, y)$}

You can see from the first expression that
\[
p(\lambda_i | \beta, y) \propto \exp(- \frac{\beta_i^2}{2 \tau^2} \lambda_i) p(\lambda_i).
\]
Letting $x_i = \lambda_i / 2$ we have that
\[
p(x_i | \beta, y) \propto \exp(- \frac{\beta_i^2}{\tau^2} x_i) p(x_i)
\]
where $x_i \sim S_{\gamma / 2}$.  So to sample $\lambda_i | \beta, y$ we do
\[
x_i \sim \texttt{retstable}(\gamma / 2, 1.0, \beta_i^2 / \tau^2)
\]
and $\lambda_i = 2 x_i$.

\subsection{Sampling $p(\beta | \lambda, \sigma^2, y)$}

The posterior for $\beta$ is
\[
p(\beta | y, \lambda, \sigma^2) \propto
\exp \Big\{ \frac{-1}{2 \sigma^2} \Big( \beta' (X'X + \frac{\sigma^2}{\tau^2}
\Lambda) \beta - 2 \beta' X' y \Big) \Big\}.
\]
Thus the precision $V^{-1}$ is
\[
V^{-1} = \frac{1}{\sigma^2} (X'X + \frac{\sigma^2}{\tau^2} \Lambda)
\]
and the mean $m$ solves
\[
X'y = \sigma^2 V^{-1} m.
\]

\subsection{Sampling $p(\sigma^2 | \beta, \lambda, y)$}

If we use an inverse-gamma prior for $\sigma^2$,
\[
p(\sigma^2) \sim IG(\text{shape}=a/2, \text{scale}=b/2)
\]
then the posterior for $\sigma^2$ is  ($\tau$ is independent of $\sigma^2$ conditional upon $\beta$)
\[
p(\sigma^2 | y, \beta) \sim IG(\text{shape}=a^*/2, \text{scale}=b^*/2)
\]
where $a^* = a + n$ and $n$ is the nubmer of observations and
\[
b^* = ||y - X \beta||^2 + b.
\]

\subsection{Sampling $p(\tau | \beta, \lambda, y)$}

First, note that neither $y$ nor $\sigma^2$ affects $\tau$ given $\beta$.  In
fact, we only need to focus on the prior.  In particular, from our work above we
have that
\[
\ell(\tau | \beta, \lambda) \propto \exp \Big( -\frac{1}{2} \frac{1}{\tau^2}
\sum_{i=1}^p \beta_i^2 \lambda_i \Big).
\]
Thus if $\tau^2$ has an $IG(a, \text{scale}=b)$ prior then the posterior is $IG(a^*,
b^*)$ where $a^* = a$ and 
\[
b^* = b + \frac{1}{2} \sum_{i=1}^p \beta_i^2 \lambda_i.
\]
Thus to sample from the posterior of $\tau^2$ we may do
\[
\begin{cases}
\phi \sim Ga(a^*, \text{rate}=b^*) \\
\tau = \sqrt{1/\phi}.
\end{cases}
\]
Note that the liklihood does not generate a probability density so we need to
use a proper prior.

\section{Mixture of Triangles MCMC}

From the Beta mixtures work, we know that
\[
p(t) = c(\alpha) \exp(-t^\alpha) = \int_0^\infty \frac{1}{\omega} \Big(1 - \frac{t}{k
  \omega}\Big)_+^{k-1} g(\omega) d\omega, \; t \geq 0
\]
where $c(\alpha)$ is the normalizing constant to make the kernel
$\exp(-t^\alpha)$ a distribution.  Thus we can generate a symmetric distribution
via
\[
p(t) = \frac{c(\alpha)}{2} \exp(-|t|^\alpha) 
= \frac{1}{2} \int_0^\infty \frac{1}{\omega} \Big(1 - \frac{|t|}{k
  \omega}\Big)_+^{k-1} g(\omega) d\omega, \; t \in \bbR.
\]
Further, we know that $g$ is defined by
\[
g(x/k) = (-1)^k \frac{x^k}{k!} f^{(k)}(x)
\]
where $f(x) = c(\alpha) \exp(-x^\alpha)$.

When $k=2$, we have 
\[
f''(x) = c(\alpha) \Big[ \alpha (1 - \alpha) x^{\alpha-2} \exp (-x^\alpha) + \alpha^2
x^{2\alpha - 2} \exp(-x^\alpha) \Big].
\]
Thus plugging into the integral above we have
\[
\int_0^\infty \frac{1}{2\omega} \Big(1 - \frac{|t|}{2 \omega}\Big)_+
\frac{(2\omega)^2}{2} f''(2 \omega) d\omega.
\]
Letting $\zeta = 2 \omega$ we have $d \zeta = 2 d \omega$ so that the integral
is
\[
\int_0^\infty \Big(1 - \frac{|t|}{\zeta}\Big)_+ \frac{\zeta}{4} f''(\zeta) d\zeta.
\]
Further
\[
\zeta f''(\zeta) = c(\alpha) \alpha \Big[ (1-\alpha) \zeta^{\alpha - 1}
\exp(\zeta^\alpha) + \alpha \zeta^{2 \alpha - 1} \exp(-\zeta^\alpha) \Big].
\]
Substituting $\xi = \zeta^\alpha$ as above we end up with $d \zeta =
\frac{1}{\alpha} \xi^{\frac{1-\alpha}{\alpha}} d \xi$ and
\[
\zeta f''(\zeta) d \zeta = c(\alpha) \Big[ (1-\alpha) \exp(-\xi) + \alpha \xi
\exp(-\xi) \Big] d \xi.
\]
Thus the integral becomes
\[
p(t) =  c(\alpha) \int_0^\infty \Big( 1 - \frac{|t|}{\xi^{1/\alpha}} \Big)_+
p(\xi) d \xi
\]
where
\[
p(\xi) = (1-\alpha) \exp(-\xi) + \alpha \xi \exp(-\xi)
\]
is a mixture of gammas.

If we think in terms of $p(t) dt$ and do the change of variables $t = s / \tau$
then $dt = \frac{ds}{\tau}$ and
\[
p(s, \xi) ds d\xi = c(\alpha) \frac{1}{\tau} \Big(1 - \frac{|s|}{\tau
  \xi^{1/\alpha}}\Big)_+ p(\xi) d \xi d s.
\]

In our MCMC, when we sample $\xi$, this will actually be truncated so that we
sample
\[
\propto \Big\{ (1-\alpha) \exp(-\xi) + \alpha \xi \exp(-\xi) \Big\} \one \{\xi
\geq a \} d\xi.
\]
By letting $\bar \xi = \xi - a$ we have
\begin{align*}
& \propto e^{-a} \Big\{(1-\alpha) \exp(
{-\bar \xi} ) + \alpha (\bar \xi + a) \exp( {-\bar \xi} )
\Big\} d \bar \xi \\
& \propto \Big\{ (\alpha a + (1-\alpha)) \exp(-\bar \xi) + \alpha \bar \xi \exp(-\bar
\xi) \Big\} d \hat \xi.
\end{align*}
Thus we have a mixture of gammas with a normalizing constant $1 + \alpha a$.

\subsection{Truncated Multivariate Normal}

Well, it appears that Rodriguez-Yam, Davis, and Scharf's method is better.  The
basic idea, as explained by James's notes is the following.

We have
\[
\beta \sim N(\hat \beta, \Sigma) \prod_{i=1}^P \one \{|\beta_j| \leq b_j \}.
\]
The basic idea, applicable to other truncation regions is to transform $\beta$
so that you are sampling from a bunch of independent random variables that have
been truncated (so they really aren't independent).  In particular, if we let $A
= \Sigma^{-1/2}$ where the square root is taken in the sense of SVD, then we
have
\[
Z = A\beta \sim N(m = A \hat \beta, \sigma^2 I)
\]
with the constraint
\[
b_j \leq \beta_j \leq b_j, \; \forall j
\]
becomes
\[
b_j \leq (A^{-1} z)_j \leq b_j, \; \forall j.
\]
We can write $(A^{-1} z)_j$ as
\[
[A^{-1}]_{ji} z_i + \sum_{k \neq i} [A^{-1}]_{jk} z_k. 
\]
Hence, in matrix notation we have
\texttt{- b[j] - AInv[j,-i] z[-i] $\leq$ AInv[j,i] z[i] $\leq$ b[j] - AInv[j,-i] z[-i]}.

I need to think about the above procedure when $\Sigma$ is not invertible, or
rather, when the precision is not invertible.  You can still write the
likelihood in that case.  I think you can maybe use the pseudo-inverse to still
do the problem, though maybe not...  Really I think it should just make one of
the variables uniform, but that's okay because of the truncation.

Update: it is better to use the SVD.  So long as the truncation is region is
bounded, you can then sample non-singular $X'X$.

We have 
\[
\exp \Big\{ \frac{-1}{2 \sigma^2} \Big( \beta' X' y - \beta' X'X \beta \Big) \Big\}
\]
constrained by
\[
- b_j \leq \beta_j \leq b_j, \; \forall j.
\]
Letting $X = UDV'$ and $Z = V'\beta$ we have that (let $A = UD$)
\[
\exp \Big\{ \frac{-1}{2 \sigma^2} \Big( Z' A'y - Z' D^2 Z \Big) \Big\}
\]
constrained by
\[
- b_j \leq (Vz)_j \leq b_j
\]
as before so that
\[
-b_j - r_{ji} \leq v_{ji} z_i \leq b_j - r_{ji}, \; \forall j
\]
where $r_{ji} = \sum_{k \neq i} v_{jk} z_k$.

When $d_i = 0$ we have that $A_{ki} = \sum_{\ell} U_{k\ell} D_{\ell i} = U_{ki}
d_i = 0$ for all $k$.  Thus the terms in the exponent drops out and we are left
with a uniform distribution for $z_j$.

\subsection*{The product of a normal and a triangle}

Things become more difficult when we try and apply this trick to the
normal-triangle product.  In that case we have
\[
\exp \Big\{ \frac{-1}{2 \sigma^2} \Big( Z' A'y - Z' D^2 Z \Big) \Big\} 
\prod_{j=1}^p \frac{1}{\kappa_j} \Big( \kappa_j - |\beta_j| \Big)_+.
\]
The product becomes a problem.  For each term we have the peicewise function
\[
\frac{1}{\kappa_j}
\begin{cases}
(\kappa_j - r_{ji} - v_{ji} z_i)_+, & v_{ji} z_i \geq - r_{ji} \\
(\kappa_j + r_{ji} + v_{ji} z_i)_+, & v_{ji} z_i \leq - r_{ji}
\end{cases}
\]
where $r_{ji} = \sum_{k \neq i} v_{jk} z_k$.  We take a product of these terms
over $j$.  Further, by taking into account the $(\cdot)_+$ function we have
\[
\frac{1}{\kappa_j}
\begin{cases}
\kappa_j - r_{ji} - v_{ji} z_i, & \kappa_j - r_{ji} \geq v_{ji} z_i \geq - r_{ji} \\
\kappa_j + r_{ji} + v_{ji} z_i, & -\kappa_j - r_{ji} \leq v_{ji} z_i \leq - r_{ji}
\end{cases}
\]
The product over $j$ is now a product of piecewise line-segments, producing a
polynomial.

So how do you sample from this?

\subsection{How can we speed this up?}

We can also write $[A^{-1}]_{j,-i} z[-i] = [A^{-1}]_{j,\cdot} z^{(cur)} -
[A^{-1}]_{j,i} z^{(cur)}_i$.  Alternatively, we can update incrementally, so
that $x = [A^{-1}]_{j,-1} z_{-1} \ra x + [A^{-1}]_{j,1} z_{1}^{(new)} -
[A^{-1}]_{j,2} z_{2}^{(old)}$.  You can preprocess $[A^{-1}]_{j,i} z_i^{(old)}$
in $i$.  But I want to be working in terms of rows.  If $A$ is symmetric that
helps us out.

We can also calculate the min and the max on the fly.

\subsection*{Sampling $p(\tau | \beta, \sigma^2, u, \omega, y)$}

First, note that we want to sample $\tau$ given everything else.  In James' and
Nick's notes they sample $\tau$ without $\omega$ and $u$.  I think it is okay to
do this so long as you sample $\omega$, $u$, and $\beta$ in the same block, but
I need to check that.  Part of the problem with that approach though is that the
prior you will use in the stable model is different from the prior you will use
for that marginal draw.  If you condition on everything then you may use the
same prior as in the stable case, but it must be proper as the liklihood for
$\tau$ does not form a valid density.

In particular, the likelihood for $\tau$ given everything else is
\[
\ell(\tau | \beta, u, \omega) \propto = \frac{1}{\tau} 
\prod_{j=1}^p \one \Big\{ \frac{|\beta_j|}{(1-u_j) \omega_j^{1/\alpha}} \leq \tau \Big\}.
\]
Let
\[
m = \max_j \frac{|\beta_j|}{(1-u_j) \omega_j^{1/\alpha}}.
\]
In the stable case we gave $\tau^2$ an inverse gamma prior.  If we do the same
thing here we have
\[
p(\tau^2 | \beta, u, \omega) \propto (\tau^2)^{-a - 1/2 - 1} \exp ( - b / \tau^2)
\one \{\tau > m\}.
\]
I believe, that if we do a change of variables $\phi = 1/\tau^2$ then we get
\[
p(\phi | \beta, u, \omega) \propto \phi^{a+1/2-1} \exp(-b \phi) \one \{\phi < 1/m^2\}.
\]
Thus we could draw from the conditional posterior of $\tau$ by
\[
\begin{cases}
\phi \sim Ga(a+1/2, b) \one_{(0, 1/m^2)} \\
\tau = \sqrt{ 1/\phi }.
\end{cases}
\]

On another note, if we look at the likelihood marginally, without $u$, we see
that
\[
p(\tau | \beta, \omega) = \frac{1}{\tau^2} 
\prod_{j=1}^p \Big( \tau - \frac{|\beta_j|}{\omega_j^{1/\alpha}} \Big)_+.
\]
I suppose we don't gain that much here.  This is actually pretty complicated
since we have a product of linear functions of $\tau$.

\section{Misc}

There are a couple ways I can draw $\beta$ in the mixture of triangles case.
The key link is
\[
xx[i,i] \hat \beta[i] = xx[i,-i] \hat \beta[-i] = xx[i,] \hat \beta = e_i xy.
\]

A couple ideas:

Why not use the mixture of uniforms representation.  Then it reduces to the
problem of sampling from a truncated normal distribution.

I think this has to do with out space changes when you increase dimensions.  It
becomes increasingly difficult to find a good proposal as the dimension
increases.

Can one use a mixture of a normal and a triangle to represent the posterior?

How can we preprocess effectively.  In the case of the uniforms the shape of the
parabola is not changing, it is just the region we are truncating that shrinks
and expands, though its shape remains the same.

What if we preprocess by sampling the normal a bunch.  Can we organize that
sample in such a way that we can pick a sample from the region we want?  Can we
use a GPU to do it.  Can we use binary search?  Can we marginalize beta to get
an idea of what the truncation regions will be?

I need to write a c-routine that samples beta.  In it I need to include the
number of iterates to go through.  That way I can approximate an independent
draw.  That will produce some nice plots.  It will also provide a nice benchmark
in terms of time.

As I get samples of the auxiliary variable in the uniform case, I can use those
to adjust my ``pool'' of samples that I have stored.

\section{RIght Truncated Gamma Distribution}

The paper by \cite{philippe-1997} shows how to sample a right truncated gamma
distribution.  However, I think we can do a better job.  She shows that a right
truncated gamma distribution can be written as an infinite mixture of beta
distributions.  Her suggestion is to use a truncation of that infinite mixture
as a proposal for an accept/reject algorithm.  However, one need not do
accept/reject, instead one may draw from the infinite mixture directly, which we
discuss below.  We will examine the case when the right truncation is at $t=1$,
since one can always rescale the random variable so that is the case.  The
parameterization we use will always be in terms of the rate of a gamma
distribution.

As shown by Philippe, the right truncated gamma distribution, $Ga(a,b,1)$ can be
written as an infinite mixture of betas.  To see this consider the truncated
gamma density
\[
f^-(x|a,b) = \frac{b^a \exp(-b)}{\gamma(a,b)} \exp(b(1-x))x^{a-1} \one_{x\leq 1}.
\]
The function $\gamma(a,b)$ is the incomplete gamma function
\[
\gamma(a,b) = \int_0^b x^{a-1} e^{-x} dx = b^a \int_0^1 x^{a-1} e^{-bx} dx.
\]
Note that
\[
\texttt{pgamma(b,a,rate=1) = pgamma(1,a,rate=b)} =: C(a,b)
\]
and
\[
C(a,b) \; \Gamma(a) = \gamma(a,b).
\]

Use the Taylor expansion of the exponential to write
\[
\exp(b(1-x)) 
= \sum_{k=0}^\infty \frac{b^k}{k!} (1-x)^k 
= \sum_{k=1}^\infty \frac{b^{k-1}}{(k-1)!} (1-x)^{k-1}. 
\]
Thus we can write the density as
\begin{align*}
f^-(x|a,b) 
& = \frac{b^a \exp(-b)}{\gamma(a,b)} 
\sum_{k=1}^\infty \frac{b^{k-1}}{\Gamma(k)} (1-x)^{k-1} x^{a-1} \one_{x \leq 1}
\\
& = \frac{b^a \exp(-b)}{\gamma(a,b)} 
\sum_{k=1}^\infty \frac{b^{k-1}}{\Gamma(k)} 
\frac{\Gamma(a) \Gamma(k)}{\Gamma(a+k)}
\frac{(1-x)^{k-1} x^{a-1}}{\beta(a,b)} \one_{x \leq 1} \\
& = \sum_{k=1}^\infty w_k f_k(x).
\end{align*}
where $f_k(x)$ is the $\beta(a,k)$ distribution and the weights are
\[
w_k = \frac{\exp(-b)}{C(a,b)} \frac{b^{a+k-1}}{\Gamma(a+k)}.
\]

\subsection{A few useful quantities}

Since we started with a distribution these weights must sum to one.  Thus
\[
\sum_{k=1}^\infty \frac{e^{-b} b^{k+a}}{\Gamma(a+k)} = C(a,b).
\]
Let's call
\[
p(k,b,a) = \frac{e^{-b} b^{k+a}}{\Gamma(a+k)};
\]
when $a$ is an integer we can interpret
\[
\frac{e^{-b} b^{k+a}}{\Gamma(a+k)}
\]
as the kernel of a truncated Poisson distribution for $j=a+k > a$.  Thus the
first $N$ terms of the sum contribute weight
\begin{align*}
\sum_{k=1}^N w_k 
& = \frac{1}{C(a,b)} 
\Big[ \sum_{k=1}^\infty p(k,b,a) - \sum_{k=N+1}^\infty p(k,b,a) \Big] \\
& = \frac{C(a,b) - C(a+N,b)}{C(a,b)} = 1 - C(a+N,b) / C(a, b).
\end{align*}

\subsection{Sampling a $Ga(a,b) \one \{x \leq 1\}$ distribution}

\textbf{Mixture of Betas}: The algoirthm is now straightforward.  To draw from
the mixture sample, first sample $k \sim \{w_k\}$.  Then sample $x \sim
\beta(a,k)$.  You can sample $k$ by sampling $u \sim \mcU(0,1)$ and then
\begin{verbatim}
k = 1;
cdf = w(1,a,b);
while(u > cdf)
  cdf += w(++k, a, b);
\end{verbatim}
Further we know that the probability of needing to calculate $N$ or fewer terms
is
\[
1 - C(a+N,b) / C(a,b).
\]

\textbf{Rejection:} If one simply does rejection sampling, i.e. draw $x \sim
Ga(a,b)$ until $x < 1$, then the probability of acceptance is $C(a,b)$.

I'm not sure how to judge just where one should make a trade-off here.  In terms
of GPU's the first is nice because it is a deterministic procedure.  But I have
no idea whether it is faster to sum 100 terms or sample 10 Gammas, for instance.
Suppose that the probability of acceptance is $\alpha$, then the expected number
of samples before accepting is (from negative binomial) $1/\alpha$.
Furthermore, it appears, looking at the \texttt{aster} package documentation
that one can calculate the expected value of a left truncated Poisson
distribution, which will let us calculate
\[
\sum_{k=1}^\infty k \frac{e^{-b} b^{k+a}}{\Gamma(k+a)}
\]
(use $k = k + a - a$).  Thus for each method we should be able to calculate the
expected number of proposals or the expected number of terms to sum.  We can
then compare those numbers to decide which method to use.

Another option, presuming we can use the quantile function.  We can calculate
$n$ so that $P(N_i \leq n) \geq 0.9$ or whatever where $N_i$ is the number of
proposals in the rejection scheme and then take $N = \gamma n$, where we scale
things up appropriately, and then calculate $1 - C(a+N,b) / C(a,b)$ to get the
probability we would use no more than $N$ terms in the sum.  Really though, we
should calculate the probability that we get the correct value within $[N, 2N]$
against the next batch of proposals.

Or we could show that one distribution dominates the other.

Another idea: Philippe, I think, uses the first $N$ terms.  It would be a better
idea to use a finite mixture centered at the terms with the most mass.

Another things to think about: One knows the mode of a Poisson distribution,
hence we at least can tell if the weights are decreasing or not.  In fact, in
reference to the last idea, we can center the finite mixture starting at the
weight associated with the mode.  The mode is related to the mean parameter
$\lambda$, which in this case is $b$.  For a non-truncated Poisson, the mode is
$\floor{b}$ and $\ceil{b}-1$.  (I gues the Poisson has two adjascent points at
the the maximum when $b$ is an integer.)  Thus if $\ceil{b}-1 \leq a$, then you
know things will be decreasing (when the first $k$ is at or above the lowest
mode).  (I am assuming that to the right of the mode a Poisson is decreasing.)

\section{Choosing $\alpha$}

We can include a Metropolis-Hastings step to model $\alpha$.  We need to
specifically state what the density for $\beta$ is to derive the likelihood for
$\alpha$.  In particular, the density is generalized exponential (or exponential
power)
\[
p(x | \mu, \tau, \alpha) = \frac{\alpha}{2 \tau \Gamma(1/\alpha)} \exp \Big(
-\Big|\frac{x-\mu}{\tau}\Big|^\alpha \Big).
\]
Thus, the likelihood for $\alpha$ is
\[
\frac{\alpha^p}{\Gamma(1/\alpha)^p} \prod_{i=1}^p \exp \Big( - |\beta_i / \tau|^\alpha \Big).
\]
Let $z_i = |\beta_i / \tau|$ and let $s_i = \log z_i \iff z_i = e^s_i$.  Then we
have
\[
\prod_{i=1}^p \exp \Big( - (e^{s_i})^{\alpha} \Big) = \exp \Big( - \sum_{i}
e^{\alpha s_i} \Big).
\]
Thus the log-likelihood is
\[
\ell(\alpha | \beta, \tau) = p \log(\alpha) - p \log (\Gamma(1/\alpha)) - \sum_{i=1}^p e^{\alpha s_i}.
\]

Our target distribution will be 
\[
q(\alpha) = \ell(\alpha) p(\alpha).
\]

Our proposal will be a simple random walk.  Pick some radius $\ep$.  Given a
centering point $y$, let $l = \max \{0, y-\ep\}$ and $r = \min \{1,y+\ep\}$.
Then the proposal $g(x|y)$ is
\[
\frac{1}{r-l} \one_{(l,r)}(x).
\]
We know, whenever drawing $x \sim g(x|y)$ that $y$ will be in the support of
$g(y|x)$.  But the two intervals may be different lenghts.  Thus we will need to
calculate $r(y)-l(y)$ and $r(x)-l(x)$.  The two intervals are guarenteed to
overlap since $x \in (y-\ep, y+\ep) \implies y \in (x-\ep, x+\ep$.  Thus the
transition will be
\[
\log g(x|y) = - \log (r(y) - l(y)).
\]

Thus the log acceptance rate will be
\[
\log q(\alpha^{new}) - \log g(\alpha^{new} | \alpha^{old}) - \log
q(\alpha^{old}) + \log g(\alpha^{old} | \alpha^{new}).
\]
Which is
\begin{align*}
& \ell(\alpha^{new} | \beta, \tau) - \ell(\alpha^{old} | \beta, \tau) \\
& p(\alpha^{new}) - p(\alpha^{old}) \\
& + \log(r(\alpha^{old}) - l(\alpha^{old})) - \log(r(\alpha^{new}) - l(\alpha^{new})).
\end{align*}
We exponentiate that and then is to check if we accept.

\newpage

\section{Empirical Benchmarks}

\begin{table}
\centering
\begin{tabular}{c c c c c c c c c}
Method & Data set & $n$ & $p$ & Time (s) & Min. & Med. & Max. & SD \\
\hline 
\multicolumn{9}{c}{Effective Sample Size} \\
Tri. & DB  & 442 &  10 & 1.51 &  9744.17 & 16583.07 & 45168.14 & 10901.50 \\
Nrm. & --- & --- & --- & 4.80 & 28727.55 & 54360.66 & 98428.62 & 24566.58 \\

Tri. & DBI & 442 &  64 & 45.12 &   227.78 &   674.99 &  1582.25 &   232.56 \\
Nrm. & --- & --- & --- & 68.68 & 17924.24 & 60873.07 & 91261.17 & 18936.41 \\

Tri. & BH  & 506 & 13  & 2.20 & 6513.61  & 46608.98 & 84133.99 & 20934.68 \\
Nrm. & --- & --- & --- & 5.96 & 17264.82 & 78399.25 & 95152.98 & 23210.93 \\

Tri. & BHI & 506 & 103 & 155.64 & 2.03 & 911.44 & 7781.48 & 1463.49 \\
Nrm. & --- & --- & --- & 197.59 & 2063.76 & 49393.23 & 100000.00 & 26360.37 \\

\multicolumn{9}{c}{Effective Sampling Rate} \\

Tri. & DB  & 442 &  10 & 1.51 & 6412.15 & 10979.52 & 29850.64 & 7212.68 \\
Nrm. & --- & --- & --- & 4.80 & 6006.14 & 11312.78 & 20522.83 & 5118.33 \\

Tri. & DBI & 442 & 64  & 45.12 &   5.06 &  14.98 &   35.10 &   5.16 \\
Nrm. & --- & --- & --- & 68.68 & 261.86 & 886.12 & 1320.26 & 276.42 \\

Tri. & BH  & 506 &  13 & 2.20 & 2964.45 & 21143.06 & 38401.99 & 9539.13 \\
Stb. & --- & --- & --- & 5.96 & 2891.94 & 13226.10 & 15957.42 & 3901.12 \\

Tri. & BHI & 506 & 103 & 155.64 & 0.01 & 5.80 & 49.99 & 9.40 \\
Nrm. & --- & --- & --- & 197.59 & 10.47 & 251.02 & 510.97 & 133.55 

\end{tabular}
\caption{
  Tri. refers to the mixture of triangles method and Nrm. refers to the precision
  mixture of normals method.  DB refers to the diabetes data found in Efron's
  LARS package, DBI refers to the diabetes data with interactions and squared
  terms included, BH refers to the Boston Housing data, and BHI refers to the
  Boston Housing data with interactions and squared terms.  For each dataset, we
  ran 10 simulations of 100,000 samples each after an initial burn-in of 10,000.
  We calculated the average effective sample size of each component of $\beta$
  over the 10 runs.  We report the minimum, median, maximum, and standard
  deviation of the
  component averages.  We implemented this code as a C routine wrapped inside
  an R package.  The tests were carried out using a MacBook Pro with 8GB of RAM and a 2 GHz Intel Core i7
  processor.  The exponentially stilted stable distributions were sampled using
  the method of \cite{devroye-2009} as implemented by the \texttt{copula} package.  Time refers to
  the execution time of our code while $n$ is the number of observations and $p$
  is dimension of $\beta$.
}
\label{tab:general}
\end{table}

\begin{table}

\centering
\begin{tabular}{c c c c c c c c c}
Method & Data set & $n$ & $p$ & Time (s) & Min. & Med. & Max. & SD \\
\hline
\multicolumn{9}{c}{Effective Sample Size} \\
Tri. & DB  & 442 &  10 & 1.20 & 56884.02 & 67798.11 & 95013.04 & 11393.50 \\
Nrm. & --- & --- & --- & 4.06 & 64337.74 & 83198.38 & 97460.85 & 10473.23 \\

Tri. & DBI & 442 &  64 &  8.69 & 20692.11 & 66531.77 & 100000 & 24770.91 \\
Nrm. & --- & --- & --- & 25.30 & 32037.12 & 76789.69 & 100000 & 20687.61 \\

Tri. & BH  & 506 &  13 &  1.62 & 59568.85 & 80129.11 & 100000 & 9164.54 \\
Nrm. & --- & --- & --- &  5.49 & 71694.94 & 96233.52 & 100000 & 7921.34 \\

Tri. & BHI & 506 & 103 & 16.22 & 42410.38 & 61372.62 & 100000 & 17878.59 \\
Nrm. & --- & --- & --- & 40.95 & 54745.48 & 76084.61 & 100000 & 16436.01 \\

\multicolumn{9}{c}{Effective Sampling Rate} \\

Tri. & DB  & 442 &  10 & 1.20 & 47343.30 & 56407.57 & 79058.63 & 9447.09 \\
Nrm. & --- & --- & --- & 4.06 & 15842.58 & 20500.59 & 23991.48 & 2581.22 \\

Tri. & DBI & 442 &  64 &  8.69 & 2377.19 & 7675.35 & 11531.89 & 2848.33 \\
Nrm. & --- & --- & --- & 25.30 & 1267.65 & 3029.24 &  3965.43 &  817.16 \\

Tri. & BH  & 506 &  13 &  1.62 & 36737.37 & 49559.42 & 61921.02 & 5711.95 \\
Nrm. & --- & --- & --- &  5.49 & 13048.85 & 17533.47 & 18198.85 & 1449.82 \\

Tri. & BHI & 506 & 103 & 16.22 & 2616.17 & 3777.34 & 6171.82 & 1102.28 \\
Nrm. & --- & --- & --- & 40.95 & 1339.81 & 1857.94 & 2441.98 &  400.95 

\end{tabular}
\caption{ We repeat the procedure described in Table \ref{tab:general} for the
  same datasets, but using a different coordinate system in which the design matrix
  is orthogonal.  When the design matrix is orthogonal it is easy to sample a
  joint draw the truncated multivariate normal distribution found in the mixture
  of triangles method.}
\label{tab:ortho}
\end{table}

We benchmark the mixture of triangles method against the mixture of normals
method to empirically assess each approach.  We use the diabetes dataset found
in the \texttt{LARS} package and the Boston housing dataset found in the
\texttt{mlbench} package.  For each data set we consider both a regression with
all of the predictors and a regression with all of the predictors plus
interactions and squared terms.  Throughout, $\sigma^2$ was given a Jeffrey's
prior, the prior for $\tau = \nu^{-1/\alpha}$ was induced by placing a
$\textmd{Ga}(2, \textmd{rate}=2)$ prior on $\nu$, and $\alpha = 0.5$.

For each data set, model, and method we ran 10 simulations.  Each simulation
generated 100,000 samples after an initial burn-in of 10,000.  We calculated the
average effective sample size and the average effective sampling rate for each
component of $\beta$ over the 10 runs.  In Table \ref{tab:general}, we report
the minimum, median, and maximum of the average effective sample size and
average effective sampling rate over the components of $\beta$ along with the
average runtime.

The effective sample size approximates the number of independent samples needed
to approximate the MCMC population.  The effective sampling rate is the
effective sample size per seconds of runtime, in other words, it is
approximately the number of independent draws produced per second.  The effective
sampling rate is the most important quantity from a practical perspective, since
it measures how quickly independent samples are created and hence the amount of
time one must take to produce a sample of a certain quality.  However, the
effective sample size is of interest from a theoretical perspective since it
gives insight into how well each Markov Chain mixes.

In addition to the general regression setting, we examined regressions with
orthogonal design matrices.  In that case, one may quickly sample from the
truncated multivariate Gaussian distribution $(\beta | u, w, y, \tau, \sigma)$
when using the mixture of triangles method since the kernel for this
distribution is a product of independent truncated normals,
\[
\exp \Big( -\frac{1}{2 \sigma^2} \beta' \beta + \frac{1}{\sigma^2} \beta X' y
\Big) \one \prod_{i=1}^p \{|\beta_i| \leq b_i \},
\]
as $X'X = I$.  In the mixture of normals case, $(\beta | \lambda, y, \tau,
\sigma)$ may be factored as a product of independent normal distributions.  We
use that structure to draw from the complete conditional of $\beta$ for a fair
comparison between the two methods.  In our numerical experiments, we use the
same datasets as in the genreal case but synthetically generate orthogonal
design matrices by transforming $\beta$ to a different coordinate system.  In
particular, we used the QR decomposition so that $X = QR$ and $\gamma = R \beta$
to do the regression $y = Q \gamma + \ep$.  We report the results for the
orthogonal design matrices in Table \ref{tab:ortho}.

We believe the best measure of performance is the minimum effective sampling
rate over the components of $\beta$.  There can be a significant amount of
variation in the effective sample sizes of the respective components.  If one is
concerned with, for instance, the sample mean of $\beta$, then the accuracy of
this quantity will be controlled by the worst effective sample size across all
components of $\beta$.  Accounting for the cost of runtime as well suggests that
one use the minimum effective sampling rate over the components of $\beta$ to
measure a sampler's utility.

The mixture of triangles method possesses some inherent disadvantages: first,
one must introduce a slice variable for the purposes of Gibbs sampling; second,
even after the introduction of that slice variable, the complete conditional of
$\beta$ is a truncated multivariate normal distribution, which is a difficult
distribution to sample and a challenging research problem in and of itself.  We
tried several different methods for sampling $\beta$, all of which alter the
original Markov Chain by, essentially, introducing a Gibbs step to produce a
draw from a chain whose marginal density is the complete conditional of $\beta$.
In particular, suppose that the Gibbs steps for the mixture of triangles method
are described the transition kernels $K_u, K_w$, $K_\beta$ where $K_\beta$ is a
draw from the truncated multivariate normal.  Ignoring $\sigma^2$ and $\tau$ for
the moment, a single pass through the Gibbs sampler can be represented by the
kernel
\[
K = K_u K_w K_\beta.
\]
Since we cannot sample $K_\beta$ directly we pick another kernel $\tilde
K_\beta$ such that
\[
 (\tilde K_\beta)^n \ra_d K_\beta,
\]
which generates a new Gibbs sampler via the transition kernel
\[
\tilde K = K_u K_w \tilde K_\beta.
\]
We tried various choices for $\tilde K_\beta$: the Gibbs method of
\cite{geweke-1991}, the transformed Gibbs method of
\cite{rodriguez-yam-etal-2004}, and the Hamilton-Monte-Carlo method of
\cite{pakman-paninski-2012}.  We found that the transformed Gibbs method of
\cite{rodriguez-yam-etal-2004} performed best.  Nonetheless, this method
introduces additional autocorrelation into the Markov Chain than otherwise would
have existed had we been able to draw from a truncated multivariate normal
directly.  In contrast, the complete conditional for $\beta$ in the mixture of
stable method is a multivariate normal distribution, which may be sampled with
relative ease.  If the mixture of normals method has one drawback, it is
sampling from the exponentially tilted stable distribution, which can be
somewhat time consuming; but \cite{devroye-2009} provides an algorithm for
sampling from this distribution that is relatively efficient.  These general
impressions were borne out in our tests.  As seen in Table \ref{tab:general} and
Table \ref{tab:ortho} the mixture of normals method has a superior effective
sample size for all of the tests, though the mixture of triangles shows
significant improvements when working with an orthogonalized design matrix.

When taking the runtime into consideration, the mixture of triangles method
fares well.  In the general regression case, when the dimension of $\beta$ is
not too large the mixture of triangles method does as well or better than the
mixture of normals method as measured by the effective sampling rate.  As the
dimension of $\beta$ grows this advantage deteriorates.  When working with an
orthogonalized design matrix, the mixture of triangle method is competitive with
the mixture of normals method even when the dimension of $\beta$ gets large.

While the effective sampling rate is the most important quantity from a
practical point of view, some caveats are in order.  Unlike effective sample
size, the runtime and hence the effective sampling rate depend upon many factors
that have nothing to do with the theoretical properties of the sampler.  For
instance, how one implements each Gibbs steps will effect the time it takes for
the program to run.  Beyond that, the language, the compiler options, if one is
using a lower level language, and the hardware can affect the running time.  In
our tests there were even slight differences in the runtime depending on whether
we compiled the C routines use R's package builder or whether we compiled them
on our own.


% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayesbridge}{}
\bibliographystyle{abbrvnat}

\end{document}

%-------------------------------------------------------------------------------

## To generate x exp(-0.5 x^2). y = 0.5 x^2.
xgrid = seq(0, 4, 0.01);
pgrid = xgrid * exp(-0.5 * xgrid^2);

Y = rexp(10000);
X = sqrt(2 * Y);
hist(X, breaks=40, prob=TRUE);
lines(xgrid, pgrid);

## Altnernatively.  Inverse CDF.
Y = runif(10000);
X = sqrt(-2 * log(1 - Y));
hist(X, breaks=40, prob=TRUE);
lines(xgrid, pgrid);

## These are actually identical because you genrate an exponential by
## rexp = - ln (runif).

## So it should be easy to do this truncated.
a = 2
b = 4
ea = exp(-0.5 * a^2);
eb = exp(-0.5 * b^2);
Z = sqrt(-2 * log(ea - (ea - eb) * Y));
par(mfrow=c(1,2))
hist(Z, breaks=20, prob=TRUE);
hist(X[X>=a & X<=b], breaks=20, prob=TRUE);

## Can you split up (x-z) exp(-0.5 x^2) and then view it as a mixture.

%-------------------------------------------------------------------------------